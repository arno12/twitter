{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import tweepy\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from settings import consumer_key, consumer_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/arnop/Documents/self_dev/twitter_api_test/scripts'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    companies = [\"blendle\", \"cafeyn\", \"milibris\", \"readly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 30 days ago - which is the maximum time Twitter allows you to go in the past.\n",
    "    date_since = datetime.now() - timedelta(days=30)\n",
    "    date_since = date_since.strftime(\"%Y-%m-%d\")\n",
    "    date_now = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    # create results folder if it doesn't exist yet\n",
    "    Path(\"../results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load previous data if it exists\n",
    "    last_31days_results_path: Path = Path(\"../results/twitter_searches_last_31_days.tsv\")\n",
    "    new_results_path = Path(\"../results/twitter_searches_incremental.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../results/twitter_searches_last_31_days.tsv')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_results_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of it is 228\n"
     ]
    }
   ],
   "source": [
    "    last_31days_results = (\n",
    "        pd.read_csv(last_31days_results_path, sep=\"\\t\")\n",
    "        if last_31days_results_path.is_file()\n",
    "        else pd.DataFrame(columns=[\"id\"])\n",
    "    )\n",
    "\n",
    "    print(f\"the length of it is {len(last_31days_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create logs folder if it doesn't exist yet\n",
    "    Path(\"../logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logs_path = Path(\"../logs/logs.csv\")\n",
    "\n",
    "    logs = (\n",
    "        pd.read_csv(logs_path)\n",
    "        if logs_path.is_file()\n",
    "        else pd.DataFrame(columns=[\"imported_at\", \"company\", \"total_rows\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initialize empty df for last 31 days - so we can add each company tweet id's to it in the loop\n",
    "    col_names = [\n",
    "        \"id\",\n",
    "        \"iso_language_code\",\n",
    "        \"created_at\",\n",
    "        \"screen_name\",\n",
    "        \"text\",\n",
    "        \"location\",\n",
    "        \"favorite_count\",\n",
    "        \"retweet_count\",\n",
    "        \"queried_at\",\n",
    "        \"company\",\n",
    "    ]\n",
    "\n",
    "    last_31days_container = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with blendle...\n",
      "original size of df: 113\n",
      "existing id's: 59\n",
      "new size of df: 54\n",
      "new length of last 31 days file is 54\n",
      "Done! Wrote a total of 54 new row(s) for blendle.\n",
      "Starting with cafeyn...\n",
      "original size of df: 130\n",
      "existing id's: 57\n",
      "new size of df: 73\n",
      "new length of last 31 days file is 127\n",
      "Done! Wrote a total of 73 new row(s) for cafeyn.\n",
      "Starting with milibris...\n",
      "original size of df: 6\n",
      "existing id's: 5\n",
      "new size of df: 1\n",
      "new length of last 31 days file is 128\n",
      "Done! Wrote a total of 1 new row(s) for milibris.\n",
      "Starting with readly...\n",
      "original size of df: 190\n",
      "existing id's: 95\n",
      "new size of df: 95\n",
      "new length of last 31 days file is 223\n",
      "Done! Wrote a total of 95 new row(s) for readly.\n"
     ]
    }
   ],
   "source": [
    "    for company in companies:\n",
    "        print(\"Starting with {}...\".format(company))\n",
    "        query = company + \" -filter:retweets\"\n",
    "\n",
    "        tweets = tweepy.Cursor(\n",
    "            api.search,\n",
    "            q=query,\n",
    "            # geocode=\"51.969685,4.051642,1000km\",\n",
    "            count=100,\n",
    "            result_type=\"recent\",\n",
    "            include_entities=True,\n",
    "            since=date_since,\n",
    "            tweet_mode=\"extended\",\n",
    "        ).items(1000)\n",
    "        locs = [\n",
    "            [\n",
    "                tweet.id,\n",
    "                tweet.metadata[\"iso_language_code\"],\n",
    "                tweet.created_at,\n",
    "                tweet.user.screen_name,\n",
    "                tweet.full_text,\n",
    "                tweet.user.location,\n",
    "                tweet.favorite_count,\n",
    "                tweet.retweet_count,\n",
    "                datetime.now(),\n",
    "                company,\n",
    "            ]\n",
    "            for tweet in tweets\n",
    "        ]\n",
    "\n",
    "        # latest data\n",
    "        df = pd.DataFrame(\n",
    "            data=locs,\n",
    "            columns=col_names,\n",
    "        )\n",
    "        print(f\"original size of df: {len(df)}\")\n",
    "\n",
    "        # Identify what values are in last_results and not in df\n",
    "        existing_ids = list(set(last_31days_results.id).intersection(df.id))\n",
    "        print(f\"existing id's: {len(existing_ids)}\")\n",
    "        # Exclude rows that contain id's that we already have from a previous iteration\n",
    "        df = df[~df.id.isin(existing_ids)]\n",
    "        print(f\"new size of df: {len(df)}\")\n",
    "\n",
    "        # Append new rows to existing result set\n",
    "        df.to_csv(\n",
    "            new_results_path,\n",
    "            mode=\"a\",\n",
    "            header=not Path(new_results_path).is_file(),\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "\n",
    "        # Upload to s3\n",
    "        upload_file(\n",
    "            \"../results/twitter_searches_incremental.tsv\",\n",
    "            \"arno12-tweets\",\n",
    "            \"all-tweets/twitter_searches_incremental.tsv\",\n",
    "        )\n",
    "\n",
    "        # Save a version with the last 31 days only\n",
    "        df_last_31_days = df[df.created_at > datetime.now() - pd.to_timedelta(\"31day\")]\n",
    "\n",
    "        last_31days_container = pd.concat([last_31days_container, df_last_31_days])\n",
    "        print(f\"new length of last 31 days file is {len(last_31days_container)}\")\n",
    "\n",
    "        # Print logs\n",
    "        print(\n",
    "            \"Done! Wrote a total of {} new row(s) for {}.\".format(\n",
    "                len(df.index), company\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Generate logs\n",
    "        logs = pd.DataFrame(\n",
    "            data=[[datetime.now().timestamp(), company, len(df.index)]],\n",
    "            columns=[\"imported_at\", \"company\", \"total_rows\"],\n",
    "        )\n",
    "\n",
    "        logs.to_csv(\n",
    "            logs_path, mode=\"a\", header=not Path(logs_path).is_file(), index=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_31days_container.to_csv(\n",
    "    '../results/twitter_searches_last_31_days.tsv',\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../results/twitter_searches_last_31_days.tsv')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iso_language_code</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>queried_at</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1414200410856898561</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-11 12:30:27</td>\n",
       "      <td>nieuwsselectie</td>\n",
       "      <td>Onbeperkt tijdschriften en het nieuws van alle...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:09.977597</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1414181907529613314</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-11 11:16:55</td>\n",
       "      <td>NadineBoke</td>\n",
       "      <td>@ikheetIngeborg @aliettejonkers @RVWetjo Ik he...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:09.977639</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1414181243395231745</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-11 11:14:17</td>\n",
       "      <td>NadineBoke</td>\n",
       "      <td>@ikheetIngeborg @aliettejonkers @RVWetjo Pffff...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:09.977659</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1414179991928123399</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-11 11:09:18</td>\n",
       "      <td>ikheetIngeborg</td>\n",
       "      <td>@NadineBoke @aliettejonkers @RVWetjo Via deze ...</td>\n",
       "      <td>The Hague</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:09.977689</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1414004388167884804</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-10 23:31:31</td>\n",
       "      <td>holofiche</td>\n",
       "      <td>Read this article from The New York Times: D.C...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:09.977706</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1411641333358399493</td>\n",
       "      <td>de</td>\n",
       "      <td>2021-07-04 11:01:35</td>\n",
       "      <td>StatementMedien</td>\n",
       "      <td>Die Juli/August-Ausgabe 2021 von [Statement], ...</td>\n",
       "      <td>Blutgasse 3, A-1010 Wien</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:19.322046</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1411640745799286790</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-04 10:59:15</td>\n",
       "      <td>SpyDeals_NL</td>\n",
       "      <td>Probeer 2 maanden gratis Readly https://t.co/C...</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:19.322057</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1411638625520046080</td>\n",
       "      <td>de</td>\n",
       "      <td>2021-07-04 10:50:49</td>\n",
       "      <td>OeJC</td>\n",
       "      <td>Die Juli/August-Ausgabe 2021 von [Statement], ...</td>\n",
       "      <td>1010 Wien, Blutgasse 3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-11 14:52:19.322068</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1411625870616412166</td>\n",
       "      <td>sv</td>\n",
       "      <td>2021-07-04 10:00:08</td>\n",
       "      <td>TommyPalomaki</td>\n",
       "      <td>Registrera dig för att få 1 månad helt gratis ...</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:19.322078</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1411614951475585025</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-04 09:16:45</td>\n",
       "      <td>LadyDeathAdder</td>\n",
       "      <td>@shanselman thinking of my mum shes T1D not su...</td>\n",
       "      <td>Naarm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-11 14:52:19.322089</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id iso_language_code          created_at  \\\n",
       "0    1414200410856898561                nl 2021-07-11 12:30:27   \n",
       "1    1414181907529613314                nl 2021-07-11 11:16:55   \n",
       "2    1414181243395231745                nl 2021-07-11 11:14:17   \n",
       "3    1414179991928123399                nl 2021-07-11 11:09:18   \n",
       "4    1414004388167884804                en 2021-07-10 23:31:31   \n",
       "..                   ...               ...                 ...   \n",
       "162  1411641333358399493                de 2021-07-04 11:01:35   \n",
       "163  1411640745799286790                en 2021-07-04 10:59:15   \n",
       "164  1411638625520046080                de 2021-07-04 10:50:49   \n",
       "165  1411625870616412166                sv 2021-07-04 10:00:08   \n",
       "166  1411614951475585025                en 2021-07-04 09:16:45   \n",
       "\n",
       "         screen_name                                               text  \\\n",
       "0     nieuwsselectie  Onbeperkt tijdschriften en het nieuws van alle...   \n",
       "1         NadineBoke  @ikheetIngeborg @aliettejonkers @RVWetjo Ik he...   \n",
       "2         NadineBoke  @ikheetIngeborg @aliettejonkers @RVWetjo Pffff...   \n",
       "3     ikheetIngeborg  @NadineBoke @aliettejonkers @RVWetjo Via deze ...   \n",
       "4          holofiche  Read this article from The New York Times: D.C...   \n",
       "..               ...                                                ...   \n",
       "162  StatementMedien  Die Juli/August-Ausgabe 2021 von [Statement], ...   \n",
       "163      SpyDeals_NL  Probeer 2 maanden gratis Readly https://t.co/C...   \n",
       "164             OeJC  Die Juli/August-Ausgabe 2021 von [Statement], ...   \n",
       "165    TommyPalomaki  Registrera dig för att få 1 månad helt gratis ...   \n",
       "166   LadyDeathAdder  @shanselman thinking of my mum shes T1D not su...   \n",
       "\n",
       "                     location favorite_count retweet_count  \\\n",
       "0                                          0             0   \n",
       "1                   Amsterdam              0             0   \n",
       "2                   Amsterdam              0             0   \n",
       "3                   The Hague              2             0   \n",
       "4                                          0             0   \n",
       "..                        ...            ...           ...   \n",
       "162  Blutgasse 3, A-1010 Wien              0             0   \n",
       "163                 Nederland              0             0   \n",
       "164    1010 Wien, Blutgasse 3              1             1   \n",
       "165                 Stockholm              0             0   \n",
       "166                     Naarm              0             0   \n",
       "\n",
       "                    queried_at  company  \n",
       "0   2021-07-11 14:52:09.977597  blendle  \n",
       "1   2021-07-11 14:52:09.977639  blendle  \n",
       "2   2021-07-11 14:52:09.977659  blendle  \n",
       "3   2021-07-11 14:52:09.977689  blendle  \n",
       "4   2021-07-11 14:52:09.977706  blendle  \n",
       "..                         ...      ...  \n",
       "162 2021-07-11 14:52:19.322046   readly  \n",
       "163 2021-07-11 14:52:19.322057   readly  \n",
       "164 2021-07-11 14:52:19.322068   readly  \n",
       "165 2021-07-11 14:52:19.322078   readly  \n",
       "166 2021-07-11 14:52:19.322089   readly  \n",
       "\n",
       "[223 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
