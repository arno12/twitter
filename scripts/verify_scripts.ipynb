{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import tweepy\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from settings import consumer_key, consumer_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glimpse(df, maxvals=10, maxlen=110):\n",
    "    print('Shape: ', df.shape)\n",
    "    \n",
    "    def pad(y):\n",
    "        max_len = max([len(x) for x in y])\n",
    "        return [x.ljust(max_len) for x in y]\n",
    "    \n",
    "    # Column Name\n",
    "    toprnt = pad(df.columns.tolist())\n",
    "    \n",
    "    # Column Type\n",
    "    toprnt = pad([toprnt[i] + ' ' + str(df.iloc[:,i].dtype) for i in range(df.shape[1])])\n",
    "    \n",
    "    # Num NAs\n",
    "    num_nas = [df.iloc[:,i].isnull().sum() for i in range(df.shape[1])]\n",
    "    num_nas_ratio = [int(round(x*100/df.shape[0])) for x in num_nas]\n",
    "    num_nas_str = [str(x) + ' (' + str(y) + '%)' for x,y in zip(num_nas, num_nas_ratio)]\n",
    "    max_len = max([len(x) for x in num_nas_str])\n",
    "    num_nas_str = [x.rjust(max_len) for x in num_nas_str]\n",
    "    toprnt = [x + ' ' + y + ' NAs' for x,y in zip(toprnt, num_nas_str)]\n",
    "    \n",
    "    # Separator\n",
    "    toprnt = [x + ' : ' for x in toprnt]\n",
    "    \n",
    "    # Values\n",
    "    toprnt = [toprnt[i] + ', '.join([str(y) for y in df.iloc[:min([maxvals,df.shape[0]]), i]]) for i in range(df.shape[1])]\n",
    "    \n",
    "    # Trim to maxlen\n",
    "    toprnt = [x[:min(maxlen, len(x))] for x in toprnt]\n",
    "    \n",
    "    for x in toprnt:\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/c/Users/arnop/Documents/self_dev/twitter_api_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    companies = [\"blendle\", \"cafeyn\", \"milibris\", \"readly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 30 days ago - which is the maximum time Twitter allows you to go in the past.\n",
    "    date_since = datetime.now() - timedelta(days=30)\n",
    "    date_since = date_since.strftime(\"%Y-%m-%d\")\n",
    "    date_now = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    # create results folder if it doesn't exist yet\n",
    "    Path(\"./results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load previous data if it exists\n",
    "    last_31days_results_path: Path = Path(\"./results/twitter_searches_last_31_days.tsv\")\n",
    "    new_results_path = Path(\"./results/twitter_searches_incremental.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_results_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of it is 375\n"
     ]
    }
   ],
   "source": [
    "last_31days_results = (\n",
    "    pd.read_csv(last_31days_results_path, sep=\"\\t\", parse_dates=['queried_at','created_at'])\n",
    "    if last_31days_results_path.is_file()\n",
    "    else pd.DataFrame(columns=[\"id\"])\n",
    ")\n",
    "\n",
    "last_31days_results = last_31days_results[last_31days_results['created_at'].apply(lambda t: t.tz_localize(None)) > (pd.to_datetime('today') - pd.to_timedelta(\"31day\"))]\n",
    "\n",
    "print(f\"the length of it is {last_31days_results.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create logs folder if it doesn't exist yet\n",
    "    Path(\"./logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logs_path = Path(\"./logs/logs.csv\")\n",
    "\n",
    "    logs = (\n",
    "        pd.read_csv(logs_path)\n",
    "        if logs_path.is_file()\n",
    "        else pd.DataFrame(columns=[\"imported_at\", \"company\", \"total_rows\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initialize empty df for last 31 days - so we can add each company tweet id's to it in the loop\n",
    "    col_names = [\n",
    "        \"id\",\n",
    "        \"iso_language_code\",\n",
    "        \"created_at\",\n",
    "        \"screen_name\",\n",
    "        \"text\",\n",
    "        \"location\",\n",
    "        \"favorite_count\",\n",
    "        \"retweet_count\",\n",
    "        \"queried_at\",\n",
    "        \"company\",\n",
    "    ]\n",
    "\n",
    "    last_31days_container = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with blendle...\n",
      "original size of new df for blendle: 99\n",
      "existing id's of blendle in last 31 days: 99\n",
      "Done! Wrote a total of 0 new row(s) for blendle\n",
      "The new length of the last 31 days file is 375\n",
      "Starting with cafeyn...\n",
      "original size of new df for cafeyn: 107\n",
      "existing id's of cafeyn in last 31 days: 107\n",
      "Done! Wrote a total of 0 new row(s) for cafeyn\n",
      "The new length of the last 31 days file is 375\n",
      "Starting with milibris...\n",
      "original size of new df for milibris: 5\n",
      "existing id's of milibris in last 31 days: 5\n",
      "Done! Wrote a total of 0 new row(s) for milibris\n",
      "The new length of the last 31 days file is 375\n",
      "Starting with readly...\n",
      "original size of new df for readly: 164\n",
      "existing id's of readly in last 31 days: 164\n",
      "Done! Wrote a total of 0 new row(s) for readly\n",
      "The new length of the last 31 days file is 375\n"
     ]
    }
   ],
   "source": [
    "    for company in companies:\n",
    "        print(\"Starting with {}...\".format(company))\n",
    "        query = company + \" -filter:retweets\"\n",
    "\n",
    "        tweets = tweepy.Cursor(\n",
    "            api.search,\n",
    "            q=query,\n",
    "            # geocode=\"51.969685,4.051642,1000km\",\n",
    "            count=100,\n",
    "            result_type=\"recent\",\n",
    "            include_entities=True,\n",
    "            since=date_since,\n",
    "            tweet_mode=\"extended\",\n",
    "        ).items(1000)\n",
    "        locs = [\n",
    "            [\n",
    "                tweet.id,\n",
    "                tweet.metadata[\"iso_language_code\"],\n",
    "                tweet.created_at,\n",
    "                tweet.user.screen_name,\n",
    "                tweet.full_text,\n",
    "                tweet.user.location,\n",
    "                tweet.favorite_count,\n",
    "                tweet.retweet_count,\n",
    "                datetime.now(),\n",
    "                company,\n",
    "            ]\n",
    "            for tweet in tweets\n",
    "        ]\n",
    "\n",
    "        # latest data\n",
    "        df = pd.DataFrame(\n",
    "            data=locs,\n",
    "            columns=col_names,\n",
    "        )\n",
    "        print(f\"original size of new df for {company}: {len(df)}\")\n",
    "\n",
    "        # Identify what values are in last_results and not in df\n",
    "        existing_ids = list(set(last_31days_results.id).intersection(df.id))\n",
    "        print(f\"existing id's of {company} in last 31 days: {len(existing_ids)}\")\n",
    "        # Exclude rows that contain id's that we already have from a previous iteration\n",
    "        new_ids = df[~df.id.isin(existing_ids)]\n",
    "\n",
    "        # Append new rows to existing result set\n",
    "        new_ids.to_csv(\n",
    "            new_results_path,\n",
    "            mode=\"a\",\n",
    "            header=not Path(new_results_path).is_file(),\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        \n",
    "        # Print logs\n",
    "        print(f\"Done! Wrote a total of {len(new_ids)} new row(s) for {company}\")\n",
    "\n",
    "        # Upload to s3\n",
    "        upload_file(\n",
    "            \"./results/twitter_searches_incremental.tsv\",\n",
    "            \"arno12-tweets\",\n",
    "            \"all-tweets/twitter_searches_incremental.tsv\",\n",
    "        )\n",
    "\n",
    "        last_31days_container = pd.concat([last_31days_container, new_ids])\n",
    "        print(f\"The new length of the last 31 days file is {len(last_31days_container)}\")\n",
    "\n",
    "        # Generate logs\n",
    "        logs = pd.DataFrame(\n",
    "            data=[[datetime.now().timestamp(), company, len(df.index)]],\n",
    "            columns=[\"imported_at\", \"company\", \"total_rows\"],\n",
    "        )\n",
    "\n",
    "        logs.to_csv(\n",
    "            logs_path, mode=\"a\", header=not Path(logs_path).is_file(), index=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_31days_container.to_csv(\n",
    "    './results/twitter_searches_last_31_days.tsv',\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iso_language_code</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>queried_at</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1416329991810387969</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-17 09:32:38</td>\n",
       "      <td>walrusit</td>\n",
       "      <td>Lees dit artikel uit De Standaard: ‘Er zou zov...</td>\n",
       "      <td>Katwijk aan Zee, NL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 12:59:53.747461</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1416314559946690560</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-17 08:31:19</td>\n",
       "      <td>nieuwsselectie</td>\n",
       "      <td>Onbeperkt tijdschriften en het nieuws van alle...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-07-17 12:59:53.747477</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1416305245924298754</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-17 07:54:18</td>\n",
       "      <td>MartijnRoyFFP</td>\n",
       "      <td>Lees dit artikel uit de Volkskrant: Als het ma...</td>\n",
       "      <td>The Netherlands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 12:59:53.747482</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416302676913106945</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-17 07:44:06</td>\n",
       "      <td>JensOldeKalter</td>\n",
       "      <td>COCAÏNE,de motor van de criminele schaduwecono...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 12:59:53.747487</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1416134386853072902</td>\n",
       "      <td>nl</td>\n",
       "      <td>2021-07-16 20:35:22</td>\n",
       "      <td>m_wiersma</td>\n",
       "      <td>Lees dit artikel uit Quote: DE MACHT VAN DE WE...</td>\n",
       "      <td>NL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 12:59:53.747491</td>\n",
       "      <td>blendle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1413422682385887236</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-09 09:00:02</td>\n",
       "      <td>PrincessMagUK</td>\n",
       "      <td>Discover the new @Dior Summer collection withi...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 13:00:00.591093</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1413417884156583936</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-09 08:40:58</td>\n",
       "      <td>LukeB_MTB</td>\n",
       "      <td>@LUDENClassics Been following the build in a c...</td>\n",
       "      <td>Bramber, probably.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 13:00:00.591098</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1413411548526227457</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-09 08:15:47</td>\n",
       "      <td>PrincessMagUK</td>\n",
       "      <td>Issue 14 of Princess Magazine is now out featu...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 13:00:00.591103</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1413410287840026624</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-09 08:10:47</td>\n",
       "      <td>RuralLifeUK</td>\n",
       "      <td>The Summer Edition of #RuralLife is Now Out! F...</td>\n",
       "      <td>South West, England</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-17 13:00:00.591108</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1413407575408726017</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-07-09 08:00:00</td>\n",
       "      <td>The_Gentleman_M</td>\n",
       "      <td>Issue 27 of The Gentleman Magazine is now out ...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-17 13:00:00.591113</td>\n",
       "      <td>readly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id iso_language_code          created_at  \\\n",
       "0    1416329991810387969                nl 2021-07-17 09:32:38   \n",
       "1    1416314559946690560                nl 2021-07-17 08:31:19   \n",
       "2    1416305245924298754                nl 2021-07-17 07:54:18   \n",
       "3    1416302676913106945                nl 2021-07-17 07:44:06   \n",
       "4    1416134386853072902                nl 2021-07-16 20:35:22   \n",
       "..                   ...               ...                 ...   \n",
       "159  1413422682385887236                en 2021-07-09 09:00:02   \n",
       "160  1413417884156583936                en 2021-07-09 08:40:58   \n",
       "161  1413411548526227457                en 2021-07-09 08:15:47   \n",
       "162  1413410287840026624                en 2021-07-09 08:10:47   \n",
       "163  1413407575408726017                en 2021-07-09 08:00:00   \n",
       "\n",
       "         screen_name                                               text  \\\n",
       "0           walrusit  Lees dit artikel uit De Standaard: ‘Er zou zov...   \n",
       "1     nieuwsselectie  Onbeperkt tijdschriften en het nieuws van alle...   \n",
       "2      MartijnRoyFFP  Lees dit artikel uit de Volkskrant: Als het ma...   \n",
       "3     JensOldeKalter  COCAÏNE,de motor van de criminele schaduwecono...   \n",
       "4          m_wiersma  Lees dit artikel uit Quote: DE MACHT VAN DE WE...   \n",
       "..               ...                                                ...   \n",
       "159    PrincessMagUK  Discover the new @Dior Summer collection withi...   \n",
       "160        LukeB_MTB  @LUDENClassics Been following the build in a c...   \n",
       "161    PrincessMagUK  Issue 14 of Princess Magazine is now out featu...   \n",
       "162      RuralLifeUK  The Summer Edition of #RuralLife is Now Out! F...   \n",
       "163  The_Gentleman_M  Issue 27 of The Gentleman Magazine is now out ...   \n",
       "\n",
       "                location favorite_count retweet_count  \\\n",
       "0    Katwijk aan Zee, NL              0             0   \n",
       "1                                     0             4   \n",
       "2        The Netherlands              0             0   \n",
       "3              Amsterdam              1             0   \n",
       "4                     NL              0             0   \n",
       "..                   ...            ...           ...   \n",
       "159       United Kingdom              0             0   \n",
       "160   Bramber, probably.              0             0   \n",
       "161       United Kingdom              0             0   \n",
       "162  South West, England              0             0   \n",
       "163       United Kingdom              0             1   \n",
       "\n",
       "                    queried_at  company  \n",
       "0   2021-07-17 12:59:53.747461  blendle  \n",
       "1   2021-07-17 12:59:53.747477  blendle  \n",
       "2   2021-07-17 12:59:53.747482  blendle  \n",
       "3   2021-07-17 12:59:53.747487  blendle  \n",
       "4   2021-07-17 12:59:53.747491  blendle  \n",
       "..                         ...      ...  \n",
       "159 2021-07-17 13:00:00.591093   readly  \n",
       "160 2021-07-17 13:00:00.591098   readly  \n",
       "161 2021-07-17 13:00:00.591103   readly  \n",
       "162 2021-07-17 13:00:00.591108   readly  \n",
       "163 2021-07-17 13:00:00.591113   readly  \n",
       "\n",
       "[374 rows x 10 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
