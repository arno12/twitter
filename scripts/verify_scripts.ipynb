{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import tweepy\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from settings import consumer_key, consumer_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glimpse(df, maxvals=10, maxlen=110):\n",
    "    print('Shape: ', df.shape)\n",
    "    \n",
    "    def pad(y):\n",
    "        max_len = max([len(x) for x in y])\n",
    "        return [x.ljust(max_len) for x in y]\n",
    "    \n",
    "    # Column Name\n",
    "    toprnt = pad(df.columns.tolist())\n",
    "    \n",
    "    # Column Type\n",
    "    toprnt = pad([toprnt[i] + ' ' + str(df.iloc[:,i].dtype) for i in range(df.shape[1])])\n",
    "    \n",
    "    # Num NAs\n",
    "    num_nas = [df.iloc[:,i].isnull().sum() for i in range(df.shape[1])]\n",
    "    num_nas_ratio = [int(round(x*100/df.shape[0])) for x in num_nas]\n",
    "    num_nas_str = [str(x) + ' (' + str(y) + '%)' for x,y in zip(num_nas, num_nas_ratio)]\n",
    "    max_len = max([len(x) for x in num_nas_str])\n",
    "    num_nas_str = [x.rjust(max_len) for x in num_nas_str]\n",
    "    toprnt = [x + ' ' + y + ' NAs' for x,y in zip(toprnt, num_nas_str)]\n",
    "    \n",
    "    # Separator\n",
    "    toprnt = [x + ' : ' for x in toprnt]\n",
    "    \n",
    "    # Values\n",
    "    toprnt = [toprnt[i] + ', '.join([str(y) for y in df.iloc[:min([maxvals,df.shape[0]]), i]]) for i in range(df.shape[1])]\n",
    "    \n",
    "    # Trim to maxlen\n",
    "    toprnt = [x[:min(maxlen, len(x))] for x in toprnt]\n",
    "    \n",
    "    for x in toprnt:\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/c/Users/arnop/Documents/self_dev/twitter_api_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    companies = [\"blendle\", \"cafeyn\", \"milibris\", \"readly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 30 days ago - which is the maximum time Twitter allows you to go in the past.\n",
    "    date_since = datetime.now() - timedelta(days=30)\n",
    "    date_since = date_since.strftime(\"%Y-%m-%d\")\n",
    "    date_now = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    # create results folder if it doesn't exist yet\n",
    "    Path(\"./results\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load previous data if it exists\n",
    "    last_31days_results_path: Path = Path(\"./results/twitter_searches_last_31_days.tsv\")\n",
    "    new_results_path = Path(\"./results/twitter_searches_incremental.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_31days_results_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1826, 10)\n",
      "id                int64     0 (0%) NAs : 1406000702153916416, 1405970732946006016, 1405966298421489664, 140591\n",
      "iso_language_code object    0 (0%) NAs : de, nl, nl, nl, nl, nl, nl, nl, nl, nl\n",
      "created_at        object    0 (0%) NAs : 2021-06-18 21:27:44+00:00, 2021-06-18 19:28:39+00:00, 2021-06-18 19:1\n",
      "screen_name       object    0 (0%) NAs : nomnomcookieez, karlomans, BlendleNL, BlendleNL, BlendleNL, BlendleNL\n",
      "text              object    0 (0%) NAs : Man kann den tollen Artikel auch bei Blendle kaufen und braucht kein \n",
      "location          object 377 (21%) NAs : nan, Maastricht, Utrecht, Utrecht, Utrecht, Utrecht, nan, Utrecht, Ut\n",
      "favorite_count    int64     0 (0%) NAs : 3, 0, 0, 0, 1, 1, 2, 1, 1, 0\n",
      "retweet_count     int64     0 (0%) NAs : 0, 0, 0, 1, 0, 0, 0, 0, 1, 1\n",
      "queried_at        object    0 (0%) NAs : 2021-06-18 19:27:44+00:00, 2021-06-18 17:28:39+00:00, 2021-06-18 17:1\n",
      "company           object    0 (0%) NAs : blendle, blendle, blendle, blendle, blendle, blendle, blendle, blendl\n"
     ]
    }
   ],
   "source": [
    "last_31days_results = (\n",
    "    pd.read_csv(last_31days_results_path, sep=\"\\t\", parse_dates=['queried_at','created_at'])\n",
    "    if last_31days_results_path.is_file()\n",
    "    else pd.DataFrame(columns=[\"id\"])\n",
    ")\n",
    "glimpse(last_31days_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-06-17 12:52:30.676744')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('today') - pd.to_timedelta(\"31day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc=pytz.UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-06-17 12:52:36.531133+0000', tz='UTC')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime((datetime.now().replace(tzinfo=pytz.UTC) - pd.to_timedelta(\"31day\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_31days_results = (\n",
    "    pd.read_csv(last_31days_results_path, sep=\"\\t\", parse_dates=[\"created_at\"])\n",
    "    if last_31days_results_path.is_file()\n",
    "    else pd.DataFrame(columns=[\"id\"])\n",
    ")\n",
    "\n",
    "glimpse(last_31days_results)\n",
    "#print(f\"the length of it is {last_31days_results.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create logs folder if it doesn't exist yet\n",
    "    Path(\"./logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logs_path = Path(\"./logs/logs.csv\")\n",
    "\n",
    "    logs = (\n",
    "        pd.read_csv(logs_path)\n",
    "        if logs_path.is_file()\n",
    "        else pd.DataFrame(columns=[\"imported_at\", \"company\", \"total_rows\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initialize empty df for last 31 days - so we can add each company tweet id's to it in the loop\n",
    "    col_names = [\n",
    "        \"id\",\n",
    "        \"iso_language_code\",\n",
    "        \"created_at\",\n",
    "        \"screen_name\",\n",
    "        \"text\",\n",
    "        \"location\",\n",
    "        \"favorite_count\",\n",
    "        \"retweet_count\",\n",
    "        \"queried_at\",\n",
    "        \"company\",\n",
    "    ]\n",
    "\n",
    "    last_31days_container = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with blendle...\n",
      "original size of new df for blendle: 92\n",
      "existing id's of blendle in last 31 days: 23\n",
      "Done! Wrote a total of 69 new row(s) for blendle\n",
      "The new length of the last 31 days file is 2438\n",
      "Starting with cafeyn...\n",
      "original size of new df for cafeyn: 103\n",
      "existing id's of cafeyn in last 31 days: 26\n",
      "Done! Wrote a total of 77 new row(s) for cafeyn\n",
      "The new length of the last 31 days file is 2515\n",
      "Starting with milibris...\n",
      "original size of new df for milibris: 5\n",
      "existing id's of milibris in last 31 days: 0\n",
      "Done! Wrote a total of 5 new row(s) for milibris\n",
      "The new length of the last 31 days file is 2520\n",
      "Starting with readly...\n",
      "original size of new df for readly: 135\n",
      "existing id's of readly in last 31 days: 21\n",
      "Done! Wrote a total of 114 new row(s) for readly\n",
      "The new length of the last 31 days file is 2634\n"
     ]
    }
   ],
   "source": [
    "    for company in companies:\n",
    "        print(\"Starting with {}...\".format(company))\n",
    "        query = company + \" -filter:retweets\"\n",
    "\n",
    "        tweets = tweepy.Cursor(\n",
    "            api.search,\n",
    "            q=query,\n",
    "            # geocode=\"51.969685,4.051642,1000km\",\n",
    "            count=100,\n",
    "            result_type=\"recent\",\n",
    "            include_entities=True,\n",
    "            since=date_since,\n",
    "            tweet_mode=\"extended\",\n",
    "        ).items(1000)\n",
    "        locs = [\n",
    "            [\n",
    "                tweet.id,\n",
    "                tweet.metadata[\"iso_language_code\"],\n",
    "                tweet.created_at,\n",
    "                tweet.user.screen_name,\n",
    "                tweet.full_text,\n",
    "                tweet.user.location,\n",
    "                tweet.favorite_count,\n",
    "                tweet.retweet_count,\n",
    "                datetime.now(),\n",
    "                company,\n",
    "            ]\n",
    "            for tweet in tweets\n",
    "        ]\n",
    "\n",
    "        # latest data\n",
    "        df = pd.DataFrame(\n",
    "            data=locs,\n",
    "            columns=col_names,\n",
    "        )\n",
    "        print(f\"original size of new df for {company}: {len(df)}\")\n",
    "\n",
    "        # Identify what values are in last_results and not in df\n",
    "        existing_ids = list(set(last_31days_results.id).intersection(df.id))\n",
    "        print(f\"existing id's of {company} in last 31 days: {len(existing_ids)}\")\n",
    "        # Exclude rows that contain id's that we already have from a previous iteration\n",
    "        new_ids = df[~df.id.isin(existing_ids)]\n",
    "\n",
    "        # Append new rows to existing result set\n",
    "        new_ids.to_csv(\n",
    "            new_results_path,\n",
    "            mode=\"a\",\n",
    "            header=not Path(new_results_path).is_file(),\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        \n",
    "        # Print logs\n",
    "        print(f\"Done! Wrote a total of {len(new_ids)} new row(s) for {company}\")\n",
    "\n",
    "        # Upload to s3\n",
    "        upload_file(\n",
    "            \"./results/twitter_searches_incremental.tsv\",\n",
    "            \"arno12-tweets\",\n",
    "            \"all-tweets/twitter_searches_incremental.tsv\",\n",
    "        )\n",
    "\n",
    "        last_31days_results = pd.concat([last_31days_results, new_ids])\n",
    "        print(\n",
    "            f\"The new length of the last 31 days file is {len(last_31days_results)}\"\n",
    "        )\n",
    "\n",
    "        # Generate logs\n",
    "        logs = pd.DataFrame(\n",
    "            data=[[datetime.now().timestamp(), company, len(df.index)]],\n",
    "            columns=[\"imported_at\", \"company\", \"total_rows\"],\n",
    "        )\n",
    "\n",
    "        logs.to_csv(\n",
    "            logs_path, mode=\"a\", header=not Path(logs_path).is_file(), index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_31days_container.to_csv(\n",
    "    './results/twitter_searches_last_31_days.tsv',\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
